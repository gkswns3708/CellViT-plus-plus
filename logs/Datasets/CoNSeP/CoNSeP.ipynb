{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare CoNSeP Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data from https://warwick.ac.uk/fac/cross_fac/tia/data/\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from scipy.io import loadmat\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set paths to dataset\n",
    "\n",
    "orig_dataset_path = Path(\"/workspace/CellViT-plus-plus/dataset/original/CoNSeP\")\n",
    "cellvit_dataset_path = Path(\"/workspace/CellViT-plus-plus/dataset/transformed/CoNSeP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Rescale Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:04<00:00,  3.03it/s]\n",
      "100%|██████████| 27/27 [00:08<00:00,  3.10it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# 1. Test images\n",
    "test_input_path = orig_dataset_path / \"Test\" / \"Images\"\n",
    "test_output_path = cellvit_dataset_path / \"Test\" / \"images\"\n",
    "test_output_path.mkdir(parents=True, exist_ok=True)\n",
    "test_image_files = list(test_input_path.glob(\"*.png\"))\n",
    "for img_file in tqdm(test_image_files):\n",
    "    loaded_image = Image.open(img_file)\n",
    "    resized = loaded_image.resize((1024, 1024), resample=Image.Resampling.LANCZOS)\n",
    "    new_img_path = test_output_path / f\"{img_file.stem}.png\"\n",
    "    resized.save(new_img_path)\n",
    "    \n",
    "# 2. Train images\n",
    "\n",
    "train_input_path = orig_dataset_path / \"Train\" / \"Images\"\n",
    "train_output_path = cellvit_dataset_path / \"Train\" / \"images\"\n",
    "train_output_path.mkdir(parents=True, exist_ok=True)\n",
    "train_image_files = tqdm(list(train_input_path.glob(\"*.png\")))\n",
    "for img_file in train_image_files:\n",
    "    loaded_image = Image.open(img_file)\n",
    "    resized = loaded_image.resize((1024, 1024), resample=Image.Resampling.LANCZOS)\n",
    "    new_img_path = train_output_path / f\"{img_file.stem}.png\"\n",
    "    resized.save(new_img_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Convert labels to numpy and rescale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:00<00:00, 22.67it/s]\n",
      "100%|██████████| 27/27 [00:01<00:00, 22.65it/s]\n"
     ]
    }
   ],
   "source": [
    "# 1. Test images\n",
    "test_input_path = orig_dataset_path / \"Test\" / \"Labels\"\n",
    "test_mask_files = list(test_input_path.glob(\"*.mat\"))\n",
    "\n",
    "test_output_path = cellvit_dataset_path / \"Test\" / \"labels-1000-1000\"\n",
    "test_output_path_resized = cellvit_dataset_path / \"Test\" / \"labels\"\n",
    "test_output_path.mkdir(parents=True, exist_ok=True)\n",
    "test_output_path_resized.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for mask_file in tqdm(test_mask_files):\n",
    "    loaded_mask= loadmat(mask_file)\n",
    "    inst_map = loaded_mask[\"inst_map\"]\n",
    "    inst_map_resized = np.array(Image.fromarray(inst_map).resize(\n",
    "        (1024, 1024), resample=Image.Resampling.NEAREST\n",
    "    )).astype(np.float64)\n",
    "    type_map = loaded_mask[\"type_map\"]\n",
    "    type_map_resized = np.array(Image.fromarray(type_map).resize(\n",
    "        (1024, 1024), resample=Image.Resampling.NEAREST\n",
    "    )).astype(np.float64)\n",
    "    output_mask = {\n",
    "        \"inst_map\": inst_map,\n",
    "        \"type_map\": type_map,\n",
    "    }\n",
    "    output_mask_resized = {\n",
    "        \"inst_map\": inst_map_resized,\n",
    "        \"type_map\": type_map_resized,\n",
    "    }\n",
    "    new_mask_path = test_output_path / f\"{mask_file.stem}.npy\"   \n",
    "    np.save(new_mask_path, output_mask)\n",
    "    new_mask_path_resized = test_output_path_resized / f\"{mask_file.stem}.npy\"\n",
    "    np.save(new_mask_path_resized, output_mask_resized)\n",
    "    \n",
    "# 2. Train images\n",
    "train_input_path = orig_dataset_path / \"Train\" / \"Labels\"\n",
    "train_mask_files = list(train_input_path.glob(\"*.mat\"))\n",
    "\n",
    "train_output_path = cellvit_dataset_path / \"Train\" / \"labels-1000-1000\"\n",
    "train_output_path_resized = cellvit_dataset_path / \"Train\" / \"labels\"\n",
    "train_output_path.mkdir(parents=True, exist_ok=True)\n",
    "train_output_path_resized.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for mask_file in tqdm(train_mask_files):\n",
    "    loaded_mask= loadmat(mask_file)\n",
    "    inst_map = loaded_mask[\"inst_map\"]\n",
    "    inst_map_resized = np.array(Image.fromarray(inst_map).resize(\n",
    "        (1024, 1024), resample=Image.Resampling.NEAREST\n",
    "    )).astype(np.float64)\n",
    "    type_map = loaded_mask[\"type_map\"]\n",
    "    type_map_resized = np.array(Image.fromarray(type_map).resize(\n",
    "        (1024, 1024), resample=Image.Resampling.NEAREST\n",
    "    )).astype(np.float64)\n",
    "    output_mask = {\n",
    "        \"inst_map\": inst_map,\n",
    "        \"type_map\": type_map,\n",
    "    }\n",
    "    output_mask_resized = {\n",
    "        \"inst_map\": inst_map_resized,\n",
    "        \"type_map\": type_map_resized,\n",
    "    }\n",
    "    new_mask_path = train_output_path / f\"{mask_file.stem}.npy\"   \n",
    "    np.save(new_mask_path, output_mask)\n",
    "    new_mask_path_resized = train_output_path_resized / f\"{mask_file.stem}.npy\"\n",
    "    np.save(new_mask_path_resized, output_mask_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['inst_map', 'type_map'])\n",
      "(1024, 1024) [  0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.\n",
      "  14.  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.\n",
      "  28.  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.\n",
      "  42.  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.\n",
      "  56.  57.  58.  59.  60.  61.  62.  63.  64.  65.  66.  67.  68.  69.\n",
      "  70.  71.  72.  73.  74.  75.  76.  77.  78.  79.  80.  81.  82.  83.\n",
      "  84.  85.  86.  87.  88.  89.  90.  91.  92.  93.  94.  95.  96.  97.\n",
      "  98.  99. 100. 101. 102. 103. 104. 105. 106. 107. 108. 109. 110. 111.\n",
      " 112. 113. 114. 115. 116. 117. 118. 119. 120. 121. 122. 123. 124. 125.\n",
      " 126. 127. 128. 129. 130. 131. 132. 133. 134. 135. 136. 137. 138. 139.\n",
      " 140. 141. 142. 143. 144. 145. 146. 147. 148. 149. 150. 151. 152. 153.\n",
      " 154. 155. 156. 157. 158. 159. 160. 161. 162. 163. 164. 165. 166. 167.\n",
      " 168. 169. 170. 171. 172. 173. 174. 175. 176. 177. 178. 179. 180. 181.\n",
      " 182. 183. 184. 185. 186. 187. 188. 189. 190. 191. 192. 193. 194. 195.\n",
      " 196. 197. 198. 199. 200. 201. 202. 203. 204. 205. 206. 207. 208. 209.\n",
      " 210. 211. 212. 213. 214. 215. 216. 217. 218. 219. 220. 221. 222. 223.\n",
      " 224. 225. 226. 227. 228. 229. 230. 231. 232. 233. 234. 235. 236. 237.\n",
      " 238. 239. 240. 241. 242. 243. 244. 245. 246. 247. 248. 249. 250. 251.\n",
      " 252. 253. 254. 255. 256. 257. 258. 259. 260. 261. 262. 263. 264. 265.\n",
      " 266. 267. 268. 269. 270. 271. 272. 273. 274. 275. 276. 277. 278. 279.\n",
      " 280. 281. 282. 283. 284. 285. 286. 287. 288. 289. 290. 291. 292. 293.\n",
      " 294. 295. 296. 297. 298. 299. 300. 301. 302. 303. 304. 305. 306. 307.\n",
      " 308. 309. 310. 311. 312. 313. 314. 315. 316. 317. 318. 319. 320. 321.\n",
      " 322. 323. 324. 325. 326. 327. 328. 329. 330. 331. 332. 333. 334. 335.\n",
      " 336. 337. 338. 339. 340. 341. 342. 343. 344. 345. 346. 347. 348. 349.\n",
      " 350. 351. 352. 353. 354. 355. 356. 357. 358. 359. 360. 361. 362. 363.\n",
      " 364. 365. 366. 367. 368. 369. 370. 371. 372. 373. 374. 375. 376. 377.\n",
      " 378. 379. 380. 381. 382. 383. 384. 385. 386. 387. 388. 389. 390. 391.\n",
      " 392. 393. 394. 395. 396. 397. 398. 399. 400. 401. 402. 403. 404. 405.\n",
      " 406. 407. 408. 409. 410. 411. 412. 413. 414. 415. 416. 417. 418. 419.\n",
      " 420. 421. 422. 423. 424. 425. 426. 427. 428. 429. 430. 431. 432. 433.\n",
      " 434. 435. 436. 437. 438. 439. 440. 441. 442. 443. 444. 445. 446. 447.\n",
      " 448. 449. 450. 451. 452. 453. 454. 455. 456. 457. 458. 459. 460. 461.\n",
      " 462. 463. 464. 465. 466. 467. 468. 469. 470. 471. 472. 473. 474. 475.\n",
      " 476. 477. 478. 479. 480. 481. 482. 483. 484. 485. 486. 487. 488. 489.\n",
      " 490. 491. 492. 493. 494. 495. 496. 497. 498. 499. 500. 501. 502. 503.\n",
      " 504. 505. 506. 507. 508. 509. 510. 511. 512. 513. 514. 515. 516. 517.\n",
      " 518. 519. 520. 521. 522. 523. 524. 525. 526. 527. 528. 529. 530. 531.\n",
      " 532. 533. 534. 535. 536. 537. 538. 539. 540. 541. 542. 543. 544. 545.\n",
      " 546. 547. 548. 549. 550. 551. 552. 553. 554. 555. 556. 557. 558. 559.\n",
      " 560. 561. 562. 563. 564. 565. 566. 567. 568. 569. 570. 571. 572. 573.\n",
      " 574. 575. 576. 577. 578. 579. 580. 581. 582. 583. 584. 585. 586. 587.\n",
      " 588. 589. 590. 591. 592. 593. 594. 595. 596. 597. 598. 599. 600. 601.\n",
      " 602. 603. 604. 605. 606. 607. 608. 609. 610. 611. 612. 613. 614. 615.\n",
      " 616. 617. 618. 619. 620. 621. 622. 623. 624. 625. 626. 627. 628. 629.\n",
      " 630. 631. 632. 633. 634. 635. 636. 637. 638. 639. 640. 641. 642. 643.\n",
      " 644. 645. 646. 647. 648. 649. 650. 651. 652. 653. 654. 655. 656. 657.\n",
      " 658. 659. 660. 661. 662. 663. 664. 665. 666. 667. 668. 669. 670. 671.\n",
      " 672. 673. 674. 675. 676. 677. 678. 679. 680. 681. 682. 683. 684. 685.\n",
      " 686. 687. 688. 689. 690. 691. 692. 693. 694. 695. 696. 697. 698. 699.\n",
      " 700. 701. 702. 703. 704. 705. 706. 707. 708. 709. 710. 711. 712. 713.\n",
      " 714. 715. 716. 717. 718. 719. 720. 721. 722. 723. 724. 725. 726. 727.\n",
      " 728. 729. 730. 731. 732. 733. 734. 735. 736. 737. 738. 739. 740. 741.\n",
      " 742. 743. 744. 745. 746. 747. 748. 749. 750. 751. 752. 753. 754. 755.\n",
      " 756. 757. 758. 759. 760. 761. 762. 763. 764. 765. 766. 767. 768. 769.\n",
      " 770. 771. 772. 773. 774. 775. 776. 777. 778. 779. 780. 781. 782. 783.\n",
      " 784. 785. 786. 787. 788. 789. 790. 791. 792. 793. 794. 795. 796. 797.\n",
      " 798. 799. 800. 801. 802. 803. 804. 805. 806. 807. 808. 809. 810. 811.\n",
      " 812. 813. 814. 815. 816. 817. 818. 819. 820. 821. 822. 823. 824. 825.\n",
      " 826. 827. 828. 829. 830. 831. 832. 833. 834. 835. 836. 837. 838. 839.\n",
      " 840. 841. 842. 843. 844. 845. 846. 847. 848. 849. 850.]\n",
      "(1024, 1024) [0. 1. 2. 4. 5.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. npy 파일 로드\n",
    "data = np.load('/workspace/CellViT-plus-plus/dataset/transformed/CoNSeP/Train/labels/train_1.npy', allow_pickle=True)\n",
    "\n",
    "# 2. 내부 딕셔너리로 변환\n",
    "label_dict = data.item()\n",
    "\n",
    "# 3. 딕셔너리 내부 키 확인 및 사용\n",
    "print(label_dict.keys())  # dict_keys(['inst_map', 'type_map'])\n",
    "\n",
    "# 4. 각 필드 출력\n",
    "print(label_dict['inst_map'].shape, np.unique(label_dict['inst_map']))\n",
    "print(label_dict['type_map'].shape, np.unique(label_dict['type_map']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exemplary configs can be found in the log folder:\n",
    "# ./logs/Classifiers/CoNSeP\n",
    "\n",
    "# python3 ./cellvit/train_cell_classifier_head.py --config /workspace/CellViT-plus-plus/config/CoNSeP_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate with consep-evaluation metrics\n",
    "# python3 ./cellvit/training/evaluate/inference_cellvit_experiment_consep.py --help\n",
    "\n",
    "\n",
    "\n",
    "# usage: inference_cellvit_experiment_consep.py [-h] [--logdir LOGDIR] [--dataset_path DATASET_PATH] [--cellvit_path CELLVIT_PATH] [--normalize_stains] [--gpu GPU]\n",
    "\n",
    "# Perform CellViT-Classifier inference for CoNSeP\n",
    "\n",
    "# options:\n",
    "#   -h, --help            show this help message and exit\n",
    "#   --logdir LOGDIR       Path to the log directory with the trained head. (default: None)\n",
    "#   --dataset_path DATASET_PATH\n",
    "#                         Path to the CoNSeP dataset (default: None)\n",
    "#   --cellvit_path CELLVIT_PATH\n",
    "#                         Path to the Cellvit model (default: None)\n",
    "#   --normalize_stains    If stains should be normalized for inference (default: False)\n",
    "#   --gpu GPU             Number of CUDA GPU to use (default: 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
